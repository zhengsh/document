# Kubernetes 高可用集群

## 概述

在之前的操作中 Kubernetes 是**集群模式**，但是在实际生产中，我们需要部署**高可用集群**，本章内容旨在完成 Kubernetes 高可用集群。

## 统一环境配置

### 节点配置

| 主机名       | IP            | 角色   | 系统              | CPU/内存 | 磁盘 |
| ------------ | ------------- | ------ | ----------------- | -------- | ---- |
| k8s-master01 | 192.168.1.100 | Master | CentOS Server 7.7 | 2核2G    | 80G  |
| k8s-master02 | 192.168.1.101 | Master | CentOS Server 7.7 | 2核2G    | 80G  |
| k8s-master03 | 192.168.1.101 | Master | CentOS Server 7.7 | 2核2G    | 80G  |
| k8s-slave01  | 192.168.1.120 | Node   | CentOS Server 7.7 | 2核4G    | 80G  |
| k8s-slave02  | 192.168.1.121 | Node   | CentOS Server 7.7 | 2核4G    | 80G  |
| k8s-slave03  | 192.168.1.122 | Node   | CentOS Server 7.7 | 2核4G    | 80G  |
| k8s vip      | 192.168.1.200 | -      | -                 | -        | -    |

## 对操作系配置

> 特别注意：以下步骤请在制作 VMware 镜像时一并完成，避免逐台安装的痛苦

#### 关闭交换空间

```bash
swapoff -a
```

#### 免开机启动交换空间

```bash
# 注释 swap 开头的行
vi /etc/fstab
```

#### 关闭防火墙

```bash
# 关闭防火墙
systemctl stop firewalld.service

# 关闭开机启动
systemctl disable firewalld.service

# 查询防火墙状态
firewall-cmd --state
```

#### 配置固定IP

```bash
# 修改为固定 ip
vi /etc/sysconfig/network-scripts/ifcfg-enp0s3

BOOTPROTO="static"         # 使用静态IP地址，默认为dhcp
IPADDR="192.168.1.xxx"     # 设置的静态IP地址
NETMASK="255.255.255.0"    # 子网掩码
GATEWAY="192.168.1.1"      # 网关地址
DNS1="192.168.1.1"         # DNS服务器
ONBOOT="yes"               # 是否开机启用

# 重启服务
service network restart
```

### 安装 Docker

```bash
# 卸载旧版本
sudo yum remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-engine
                  
# 1. 安装必要的一些系统工具
sudo yum install -y yum-utils device-mapper-persistent-data lvm2
# 2. 添加软件源信息
sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# 3. 更新并安装Docker-CE
sudo yum makecache fast
sudo yum -y install docker-ce
# 4. 开启Docker服务和设置开机启动
sudo systemctl start docker
sudo systemctl enable docker
```

### 配置 Docker 加速器

> 特别注意：国内镜像加速器可能会很卡，请替换成你自己阿里云镜像加速器，地址如：`https://3ogo1qyx.mirror.aliyuncs.com`，在阿里云控制台的 **容器镜像服务 -> 镜像加速器** 菜单中可以找到

在 `/etc/docker/daemon.json` 中写入如下内容（如果文件不存在请新建该文件）

```json
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://3ogo1qyx.mirror.aliyuncs.com"]
}
EOF
```

### 安装 kubeadm，kubelet，kubectl

```bash
# 证书安装
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# 安装 kubeadm，kubelet，kubectl
yum install -y kubectl kubelet kubeadm

systemctl enable kubelet
```

### 设置时区和时间

```shell
# 设置时区（同步时间前先设置）
timedatectl set-timezone Asia/Shanghai

# 安装 ntp 组件
yum -y install ntp

systemctl enable ntpd
systemctl start ntpd

# 同步时间
ntpdate -u cn.pool.ntp.org

# 查询时间
date 
# 查询结果
Wed Jul 15 22:19:12 CST 2020
```

### 配置 IPVS

```shell
# 安装系统工具
yum install -y ipset ipvsadm

# 配置并加载 IPVS 模块
mkdir -p /etc/sysconfig/modules/
vim /etc/sysconfig/modules/ipvs.modules

# 输入如下内容
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4


# 执行脚本，注意：如果重启则需要重新运行该脚本
chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4

# 执行脚本输出如下
ip_vs_sh               12688  0 
ip_vs_wrr              12697  0 
ip_vs_rr               12600  4 
ip_vs                 145497  10 ip_vs_rr,ip_vs_sh,ip_vs_wrr
nf_conntrack_ipv4      15053  19 
nf_defrag_ipv4         12729  1 nf_conntrack_ipv4
nf_conntrack          139264  9 ip_vs,nf_nat,nf_nat_ipv4,nf_nat_ipv6,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_netlink,nf_conntrack_ipv4,nf_conntrack_ipv6
libcrc32c              12644  4 xfs,ip_vs,nf_nat,nf_conntrack
```

### 配置内核参数

```shell
# 配置参数
vim /etc/sysctl.d/k8s.conf

# 输入如下内容
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_nonlocal_bind = 1
net.ipv4.ip_forward = 1
vm.swappiness=0


# 应用参数
sysctl --system

# 应用参数输出如下（找到 Applying /etc/sysctl.d/k8s.conf 开头的日志）
* Applying /usr/lib/sysctl.d/00-system.conf ...
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0
* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
kernel.yama.ptrace_scope = 0
* Applying /usr/lib/sysctl.d/50-default.conf ...
kernel.sysrq = 16
kernel.core_uses_pid = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.default.promote_secondaries = 1
net.ipv4.conf.all.promote_secondaries = 1
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/k8s.conf ...
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_nonlocal_bind = 1
net.ipv4.ip_forward = 1
vm.swappiness = 0
* Applying /etc/sysctl.conf ...
```

## 单独节点配置

> 特别注意：为 Master 和 Node 节点单独配置对应的 **IP** 和 **主机名**

### 配置 IP

```shell
# 修改为固定 ip
vi /etc/sysconfig/network-scripts/ifcfg-enp0s3

BOOTPROTO="static"         # 使用静态IP地址，默认为dhcp
IPADDR="192.168.1.xxx"     # 设置的静态IP地址
NETMASK="255.255.255.0"    # 子网掩码
GATEWAY="192.168.1.1"      # 网关地址
DNS1="192.168.1.1"         # DNS服务器
ONBOOT="yes"               # 是否开机启用

# 重启服务
service network restart
```

### 配置主机名

```bash
# 修改主机名
hostnamectl set-hostname k8s-master03

# 配置 hosts
cat >> /etc/hosts << EOF
192.168.1.100 k8s-master01
192.168.1.101 k8s-master02
192.168.1.102 k8s-master03
EOF
```

## 安装 HAProxy + Keepalived

### 概述

Kubernetes Master 节点运行组件如下：

- **kube-apiserver：** 提供了资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制
- **kube-scheduler：** 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上
- **kube-controller-manager：** 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等
- **etcd：** CoreOS 基于 Raft 开发的分布式 key-value 存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）

`kube-scheduler` 和 `kube-controller-manager` 可以以集群模式运行，通过 leader 选举产生一个工作进程，其它进程处于阻塞模式。

**`kube-apiserver` 可以运行多个实例，但对其它组件需要提供统一的访问地址，本章节部署 Kubernetes 高可用集群实际就是利用 HAProxy + Keepalived 配置该组件**

配置的思路就是利用 HAProxy + Keepalived 实现 `kube-apiserver` 虚拟 IP 访问从而实现高可用和负载均衡，拆解如下：

- Keepalived 提供 `kube-apiserver` 对外服务的虚拟 IP（VIP）

- HAProxy 监听 Keepalived VIP

- 运行 Keepalived 和 HAProxy 的节点称为 LB（负载均衡） 节点

- Keepalived 是一主多备运行模式，故至少需要两个 LB 节点

- Keepalived 在运行过程中周期检查本机的 HAProxy 进程状态，如果检测到 HAProxy 进程异常，则触发重新选主的过程，VIP 将飘移到新选出来的主节点，从而实现 VIP 的高可用

- 所有组件（如 kubeclt、apiserver、controller-manager、scheduler 等）都通过 VIP +HAProxy 监听的 6444 端口访问 `kube-apiserver` 服务（**注意：`kube-apiserver` 默认端口为 6443，为了避免冲突我们将 HAProxy 端口设置为 6444，其它组件都是通过该端口统一请求 apiserver**）

  ![](..\images\service-mesh\Kubernetes_ HK.png)

### 创建 HAProxy 启动脚本

> 该步骤在 `kubernetes-master-01` 执行

```shell
mkdir -p /usr/local/docker/kubernetes/lb
vi /usr/local/docker/kubernetes/lb/start-haproxy.sh

# 输入内容如下
#!/bin/bash
# 修改为你自己的 Master 地址
MasterIP1=192.168.1.100
MasterIP2=192.168.1.101
MasterIP3=192.168.1.102
# 这是 kube-apiserver 默认端口，不用修改
MasterPort=6443

# 容器将 HAProxy 的 6444 端口暴露出去
docker run -d --restart=always --name HAProxy-K8S -p 6444:6444 \
        -e MasterIP1=$MasterIP1 \
        -e MasterIP2=$MasterIP2 \
        -e MasterIP3=$MasterIP3 \
        -e MasterPort=$MasterPort \
        wise2c/haproxy-k8s

# 设置权限
chmod +x start-haproxy.sh
```

### 创建 Keepalived 启动脚本

> 该步骤在 `kubernetes-master-01` 执行

```shell
mkdir -p /usr/local/docker/kubernetes/lb && cd /usr/local/docker/kubernetes/lb
vi /usr/local/docker/kubernetes/lb/start-keepalived.sh

# 输入内容如下
#!/bin/bash
# 修改为你自己的虚拟 IP 地址
VIRTUAL_IP=192.168.1.200
# 虚拟网卡设备名
INTERFACE=ens33
# 虚拟网卡的子网掩码
NETMASK_BIT=24
# HAProxy 暴露端口，内部指向 kube-apiserver 的 6443 端口
CHECK_PORT=6444
# 路由标识符
RID=10
# 虚拟路由标识符
VRID=160
# IPV4 多播地址，默认 224.0.0.18
MCAST_GROUP=224.0.0.18

docker run -itd --restart=always --name=Keepalived-K8S \
        --net=host --cap-add=NET_ADMIN \
        -e VIRTUAL_IP=$VIRTUAL_IP \
        -e INTERFACE=$INTERFACE \
        -e CHECK_PORT=$CHECK_PORT \
        -e RID=$RID \
        -e VRID=$VRID \
        -e NETMASK_BIT=$NETMASK_BIT \
        -e MCAST_GROUP=$MCAST_GROUP \
        wise2c/keepalived-k8s

# 设置权限
chmod +x start-keepalived.sh

```

### 复制脚本到其它 Master 地址

分别在 `kubernetes-master-02` 和 `kubernetes-master-03` 执行创建工作目录命令

```shell
mkdir -p /usr/local/docker/kubernetes/lb
```

将 `kubernetes-master-01` 中的脚本拷贝至其它 Master

```shell
scp start-haproxy.sh start-keepalived.sh 192.168.1.101:/usr/local/docker/kubernetes/lb
scp start-haproxy.sh start-keepalived.sh 192.168.1.102:/usr/local/docker/kubernetes/lb
```

分别在 3 个 Master 中启动容器（执行脚本）

```shell'
sh /usr/local/docker/kubernetes/lb/start-haproxy.sh && sh /usr/local/docker/kubernetes/lb/start-keepalived.sh
```

### 验证是否成功过

```shell
docker ps


# 输出如下
5116a5813c0f        wise2c/keepalived-k8s   "/usr/bin/keepalived…"   11 seconds ago      Up 8 seconds                                 Keepalived-K8S
885b94a99df9        wise2c/haproxy-k8s      "/docker-entrypoint.…"   44 seconds ago      Up 38 seconds       0.0.0.0:6444->6444/tcp   HAProxy-K8S
```

### 查看网卡绑定的虚拟 IP

```shell
ip a | grep ens33

# 输出如下
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    inet 192.200.1.224/24 brd 192.200.1.255 scope global noprefixroute enp0s3
    inet 192.200.1.230/24 scope global secondary enp0s3
```

> 特别注意：Keepalived 会对 HAProxy 监听的 6444 端口进行检测，如果检测失败即认定本机 HAProxy 进程异常，会将 VIP 漂移到其他节点，所以无论本机 Keepalived 容器异常或 HAProxy 容器异常都会导致 VIP 漂移到其他节点

## 部署 Kubernetes 集群

### 初始化 Master

* 创建工作目录并且导出配置文件

```shell
# 创建工作目录
mkdir -p /usr/local/kubernetes/cluster

# 导出配置文件到工作目录
kubeadm config print init-defaults --kubeconfig ClusterConfiguration > kubeadm.yml
```

- 修改配置文件

```shell
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  # Token 永不过期
  ttl: 0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  # 修改为主节点 IP
  advertiseAddress: 192.168.1.100
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: kubernetes-master
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
# 配置 Keepalived 地址和 HAProxy 端口
controlPlaneEndpoint: "192.168.1.200:6444"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
# 国内不能访问 Google，修改为阿里云
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
# 修改版本号
kubernetesVersion: v1.18.0
networking:
  dnsDomain: cluster.local
  # 主要修改在这里，替换 Calico 网段为我们虚拟机不重叠的网段（这里用的是 Flannel 默认网段）
  podSubnet: "10.244.0.0/16"
  # 配置成 Calico 的默认网段
  #podSubnet: "192.168.0.0/16"
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
# 开启 IPVS 模式
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs

```

- kubeadm 初始化

```shell
# kubeadm 初始化
kubeadm init --config=kubeadm.yml --upload-certs | tee kubeadm-init.log

# 完成如下
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 192.168.1.200:6444 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:a7ef74025ff79a2be52f5132d968c6b437bd986701fea033da34083adcff2a50 \
    --control-plane --certificate-key 902ab630a26d9521282f86c6352960010ad4c583d04d9fda625e86544583ea66

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.1.200:6444 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:a7ef74025ff79a2be52f5132d968c6b437bd986701fea033da34083adcff2a50 
# 配置 kubectl
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

# 验证是否成功
kubectl get node

# 返回结果
NAME           STATUS   ROLES    AGE   VERSION
k8s-master01   Ready    master   76s   v1.18.6

# 确认安装成功
kubectl get pods --all-namespaces

# 等到所有状态为 Running, 注意时间可能比较久， 3-4分钟
NAMESPACE     NAME                                       READY   STATUS     RESTARTS   AGE
kube-system   calico-kube-controllers-578894d4cd-qshpj   1/1     Running    0          14m
kube-system   calico-node-hlcb2                          1/1     Running    0          68s
kube-system   calico-node-mrxkg                          0/1     Init:2/3   0          37s
kube-system   calico-node-sbzbt                          1/1     Running    0          46m
kube-system   coredns-7ff77c879f-5mwvk                   1/1     Running    0          14m
kube-system   coredns-7ff77c879f-wqzhq                   1/1     Running    0          14m
kube-system   etcd-k8s-master                            1/1     Running    1          23h
kube-system   kube-apiserver-k8s-master                  1/1     Running    1          23h
kube-system   kube-controller-manager-k8s-master         1/1     Running    1          23h
kube-system   kube-proxy-b5gnq                           1/1     Running    1          68s
kube-system   kube-proxy-mq9r5                           1/1     Running    1          37s
kube-system   kube-proxy-wnr7l                           1/1     Running    3          23h
kube-system   kube-scheduler-k8s-master                  1/1     Running    2          23h
```

* 安装网络插件

```shell
# 安装 Calico
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
wget https://docs.projectcalico.org/manifests/calico.yaml

#修改第 611 行，将 192.168.0.0/16 修改为 10.244.0.0/16，可以通过如下命令快速查找

#显示行号：:set number
#查找字符：/要查找的字符，输入小写 n 下一个匹配项，输入大写 N 上一个匹配项
# The default IPv4 pool to create on startup if none exists. Pod IPs will be
# chosen from this range. Changing this value after installation will have
# no effect. This should fall within `--cluster-cidr`.
		- name: CALICO_IPV4POOL_CIDR
			value: "10.244.0.0/16"
			
# 安装
kubectl apply -f calico.yaml

# 验证安装是否成功
configmap/calico-config unchanged
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org configured
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers unchanged
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers unchanged
clusterrole.rbac.authorization.k8s.io/calico-node unchanged
clusterrolebinding.rbac.authorization.k8s.io/calico-node unchanged
daemonset.apps/calico-node configured
serviceaccount/calico-node unchanged
deployment.apps/calico-kube-controllers unchanged
serviceaccount/calico-kube-controllers unchanged
```

### 加入 Master 节点

从 `kubeadm-init.log` 中获取命令，分别将 `kubernetes-master-02` 和 `kubernetes-master-03` 加入 Master

```shell
# 以下为示例命令
kubeadm join 192.168.1.200:6444 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:48c4996400e4232b4682118f205bcc92fe6f0d376d059d941b63497bd9424717 \
    --control-plane --certificate-key f37c206ce568ee3a75bf871276dbe01caada37201b669c3b42cc6ebf6ab0e783
```

### 加入 Node 节点

从 `kubeadm-init.log` 中获取命令，分别将 `kubernetes-node-01` 至 `kubernetes-node-03` 加入 Node

```shell
# 以下为示例命令
kubeadm join 192.168.1.200:6444 --token abcdef.0123456789abcdef \
    --discovery-token-ca-cert-hash sha256:48c4996400e4232b4682118f205bcc92fe6f0d376d059d941b63497bd9424717

```

### 验证集群状态

* 查看 Node

```shel
kubectl get nodes -o wide
```

* 查看 Pod

```shell
kubectl -n kube-system get pod -o wide
```

* 查看 Service

```shell
kubectl -n kube-system get svc
```

* 验证 IPVS

  查看 kube-proxy 日志，server_others.go:176] Using ipvs Proxier.

```shell
kubectl -n kube-system logs -f <kube-proxy 容器名>
```

* 查看代理规则

```shell
ipvsadm -ln
```

* 查看 etcd 集群

```shell
kubectl -n kube-system exec etcd-k8s-master01 -- etcdctl \
	--endpoints=https://192.168.1.100:2379 \
	--ca-file=/etc/kubernetes/pki/etcd/ca.crt \
	--cert-file=/etc/kubernetes/pki/etcd/server.crt \
	--key-file=/etc/kubernetes/pki/etcd/server.key cluster-health

# 输出如下
member 1dfaf07371bb0cb6 is healthy: got healthy result from https://192.168.141.152:2379
member 2da85730b52fbeb2 is healthy: got healthy result from https://192.168.141.150:2379
member 6a3153eb4faaaffa is healthy: got healthy result from https://192.168.141.151:2379
cluster is healthy
```

### 验证高可用

特别注意：Keepalived 要求至少 2 个备用节点，故想测试高可用至少需要 1 主 2 从模式验证，否则可能出现意想不到的问题

对任意一台 Master 机器执行关机操作

```shell
shutdown -h now
```

在任意一台 Master 节点上查看 Node 状态

```shell
kubectl get node

# 输出如下，除已关机那台状态为 NotReady 其余正常便表示成功
NAME                   STATUS   ROLES    AGE   VERSION
kubernetes-master-01   NotReady master   18m   v1.14.2
kubernetes-master-02   Ready    master   17m   v1.14.2
kubernetes-master-03   Ready    master   16m   v1.14.2
```

查看 VIP 漂移

```shell
ip a |grep ens33

# 输出如下
2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    inet 192.168.141.151/24 brd 192.168.141.255 scope global ens33
    inet 192.168.141.200/24 scope global secondary ens33
```

### 设置host

```shell
cat >> /etc/hosts << EOF
192.200.1.220    k8s-master01
192.200.1.223    k8s-master02
192.200.1.224    k8s-master0
EOF
```

## Docker Haproxy + Keepalived 关闭

```shell
#停止所有的container，这样才能够删除其中的images：
docker stop $(docker ps -a -q)

#如果想要删除所有container的话再加一个指令：
docker rm $(docker ps -a -q)

#删除全部image的话
docker rmi $(docker images -q)


#合并执行
docker stop $(docker ps -a -q) && docker rm $(docker ps -a -q) && docker rmi $(docker images -q)
```

 